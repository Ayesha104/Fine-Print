# -*- coding: utf-8 -*-
"""Scraper for privacy policies.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zb2TCWlaHqI9J1MHpVVGjGPY6fRl_Kb8
"""

!pip install html2text

!pip install langdetect

!pip install inscriptis

!pip install python-Levenshtein
!pip install fuzzywuzzy
from fuzzywuzzy import fuzz

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

import xml.etree.ElementTree as ET
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import spacy
import html2text
import re
import requests
from bs4 import BeautifulSoup 
import pandas as pd
from langdetect import detect
from urllib import request
from inscriptis import get_text
from nltk.tokenize import RegexpTokenizer
from socket import timeout
import urllib
import ssl

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

mapsLinks = pd.read_csv('/content/drive/My Drive/Colab Notebooks/mapsLinks.csv') 
mapsLinks = mapsLinks["0"].tolist()

def replace(string, substitutions):
    substrings = sorted(substitutions, key=len, reverse=True)
    regex = re.compile('|'.join(map(re.escape, substrings)))
    return regex.sub(lambda match: substitutions[match.group(0)], string)

def is_valid(text):
  try:
    if(detect(text) != "en"):
      return False
  except:
    return False
  if (fuzz.token_set_ratio(text,"HTTP Error 404: Not Found") >60):
    return False

  tokenizer = RegexpTokenizer(r'\w+')
  tokens = tokenizer.tokenize(text)
  #print("#words: ", len(tokens))
  if len(tokens) < 80:
    return False
  return True

import http
import lxml

for url in mapsLinks:
  try:
    ht=request.urlopen(url,timeout=60)
    html_ = ht.read().decode('utf-8','ignore')
    text = get_text(html_)
    substitutions = {"  ": "", "\\n": "", "\\r": ""}
    text = replace(text, substitutions)
    if(is_valid(text)):
      filepath = os.path.join("/content/drive/My Drive/Colab Notebooks/FYPDS2/"+str(i)+".txt")
      file1 = open(filepath,"w",encoding="utf-8") 
      file1.write(text)
      file1.close()
      filepath = os.path.join("/content/drive/My Drive/Colab Notebooks/FYPDS2html/"+str(i)+"html.txt")
      file1 = open(filepath,"w") 
      file1.write(html_)
      file1.close()
    i+=1

  except urllib.error.HTTPError as e:
    print(url)
    print(i)
    print('HTTPError = ' + str(e.code))
    fileError = open("errorIndex2.txt", "a+") #storing the error indices
    fileError.write(str(i)+"\n")
    fileError.close()
    i+=1
  except urllib.error.URLError as e:
    print(url)
    print(i)
    print('URLError = ' + str(e.reason))
    fileError = open("errorIndex2.txt", "a+")
    fileError.write(str(i)+"\n")
    fileError.close()
    i+=1
  except timeout:
    print(url)
    print(i)
    print('TimeoutError ')
    fileError = open("errorIndex2.txt", "a+")
    fileError.write(str(i)+"\n")
    fileError.close()
    i+=1    
  except http.client.HTTPException as e:
    print(url)
    print(i)
    print('BadLineError')
    fileError = open("errorIndex2.txt", "a+")
    fileError.write(str(i)+"\n")
    fileError.close()
    i+=1 
  except ssl.CertificateError as e:
    print ("ssl certificateError: ", e)
    i+=1
  except lxml.etree.ParserError as e:
    print ("ParseError: ", e)
    i+=1